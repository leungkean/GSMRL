{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bcdf93",
   "metadata": {},
   "source": [
    "# Clean Data For Q-Learning and GSMRL Model\n",
    "Here we clean the data by filling the missing values using the previous values in the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c23532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55554f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0Metal device set to: Apple M1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GPU setup\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "seed = 123\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b47646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_points = 10000\\ntest_size  = int(data_points*0.1)\\nvalid_size = test_size\\ntrain_size = data_points - 2 * test_size\\n\\nindices = np.arange(data_points)\\nrng.shuffle(indices)\\ntrain_indices = indices[: train_size]\\nvalidation_indices = indices[train_size : -valid_size]\\ntest_indices = indices[-test_size :]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get indices for training, validation, and testing\n",
    "\"\"\"\n",
    "data_points = 10000\n",
    "test_size  = int(data_points*0.1)\n",
    "valid_size = test_size\n",
    "train_size = data_points - 2 * test_size\n",
    "\n",
    "indices = np.arange(data_points)\n",
    "rng.shuffle(indices)\n",
    "train_indices = indices[: train_size]\n",
    "validation_indices = indices[train_size : -valid_size]\n",
    "test_indices = indices[-test_size :]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499b17c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t8/xpt23s1j08vd186gztkz58700000gn/T/ipykernel_58609/2990347107.py:1: DtypeWarning: Columns (10,13,14,21,24,25,26,27,28,30,33,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pandas.read_csv(\"./Wright/Daily Diary Long Form.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Datessmt</th>\n",
       "      <th>True_date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>True_date_b</th>\n",
       "      <th>TimeB</th>\n",
       "      <th>WeekdayB</th>\n",
       "      <th>WeekendB</th>\n",
       "      <th>...</th>\n",
       "      <th>LOV</th>\n",
       "      <th>NegAffDay</th>\n",
       "      <th>Urgency</th>\n",
       "      <th>Exhibitionism</th>\n",
       "      <th>DetatchDay</th>\n",
       "      <th>ImpulsivityDay</th>\n",
       "      <th>Compulsivity</th>\n",
       "      <th>PsychoDay</th>\n",
       "      <th>HostileDay</th>\n",
       "      <th>ManipDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2/11/13</td>\n",
       "      <td>2/11/13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2/11/13</td>\n",
       "      <td>2/12/13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2/11/13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2/11/13</td>\n",
       "      <td>2/13/13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2/12/13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.95</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>6.33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2/11/13</td>\n",
       "      <td>2/14/13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2/13/13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "      <td>6</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2/11/13</td>\n",
       "      <td>2/15/13</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2/14/13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.67</td>\n",
       "      <td>6</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>116.0</td>\n",
       "      <td>3/19/13</td>\n",
       "      <td>6/13/13</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6/12/13</td>\n",
       "      <td>85</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>116.0</td>\n",
       "      <td>3/19/13</td>\n",
       "      <td>6/14/13</td>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6/13/13</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>116.0</td>\n",
       "      <td>3/19/13</td>\n",
       "      <td>6/15/13</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6/14/13</td>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>116.0</td>\n",
       "      <td>3/19/13</td>\n",
       "      <td>6/15/13</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6/15/13</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>116.0</td>\n",
       "      <td>3/19/13</td>\n",
       "      <td>6/15/13</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6/15/13</td>\n",
       "      <td>88</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Datessmt True_date Time Weekday Weekend True_date_b TimeB  \\\n",
       "0       1.0  2/11/13   2/11/13    1       1       0         NaN   NaN   \n",
       "1       1.0  2/11/13   2/12/13    2       2       0     2/11/13     1   \n",
       "2       1.0  2/11/13   2/13/13    3       3       0     2/12/13     2   \n",
       "3       1.0  2/11/13   2/14/13    4       4       0     2/13/13     3   \n",
       "4       1.0  2/11/13   2/15/13    5       5       0     2/14/13     4   \n",
       "...     ...      ...       ...  ...     ...     ...         ...   ...   \n",
       "9995  116.0  3/19/13   6/13/13   86       4       0     6/12/13    85   \n",
       "9996  116.0  3/19/13   6/14/13   87       5       0     6/13/13    86   \n",
       "9997  116.0  3/19/13   6/15/13   88       6       1     6/14/13    87   \n",
       "9998  116.0  3/19/13   6/15/13   88       6       1     6/15/13    88   \n",
       "9999  116.0  3/19/13   6/15/13   88       6       1     6/15/13    88   \n",
       "\n",
       "     WeekdayB WeekendB  ...    LOV  NegAffDay  Urgency Exhibitionism  \\\n",
       "0         NaN      NaN  ...  -7.95       3.25        0          0.33   \n",
       "1           1        0  ...  -3.66        4.5      1.5          0.33   \n",
       "2           2        0  ...  -5.95       3.75        1          0.33   \n",
       "3           3        0  ...  -2.41       4.75        1          0.67   \n",
       "4           4        0  ...  -1.83        4.5      1.5          0.67   \n",
       "...       ...      ...  ...    ...        ...      ...           ...   \n",
       "9995        3        0  ...  -7.95       3.75        0             0   \n",
       "9996        4        0  ...  -1.41        2.5        0             0   \n",
       "9997        5        0  ...  -1.83       2.25        0          0.67   \n",
       "9998        6        1  ...  -1.83       2.25        0          0.67   \n",
       "9999        6        1  ...  -1.83       2.25        0          0.67   \n",
       "\n",
       "     DetatchDay ImpulsivityDay  Compulsivity  PsychoDay  HostileDay  ManipDay  \n",
       "0          6.33           0.67          0.33       3.17           0         0  \n",
       "1          4.67              1          1.67       3.17         0.5         0  \n",
       "2          6.33              1             1       4.67        0.25         1  \n",
       "3             6           0.33          0.67       4.67        0.25       0.5  \n",
       "4             6           0.33             1       3.67        0.25       0.5  \n",
       "...         ...            ...           ...        ...         ...       ...  \n",
       "9995          3              0          0.33       1.67           0         0  \n",
       "9996          2              0             0       0.83           0         0  \n",
       "9997       1.67              0             0        1.5           0         0  \n",
       "9998       1.67              0             0        1.5           0         0  \n",
       "9999       1.67              0             0        1.5           0         0  \n",
       "\n",
       "[10000 rows x 210 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"./Wright/Daily Diary Long Form.csv\")\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df.fillna(method='ffill')\n",
    "features = df.loc[:,\"Stress1\":\"LOV\"]\n",
    "time     = df.loc[:, \"Time\"]\n",
    "target   = df.loc[:, \"NegAffDay\":]\n",
    "ind_demo = df.loc[:, \"Gender\":\"NEO_C\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b34f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "for i in range(1,81):\n",
    "    train_indices += df.index[df['Time'] == f'{i}'].tolist()\n",
    "validation_indices = []\n",
    "for i in range(80,91):\n",
    "    validation_indices += df.index[df['Time'] == f'{i}'].tolist()\n",
    "test_indices = []\n",
    "for i in range(90,103):\n",
    "    test_indices += df.index[df['Time'] == f'{i}'].tolist()\n",
    "\n",
    "train_indices      = np.array(train_indices, dtype=np.int64)\n",
    "validation_indices = np.array(validation_indices, dtype=np.int64)\n",
    "test_indices       = np.array(test_indices, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8725c1",
   "metadata": {},
   "source": [
    "# Binarize Feature Data Based Median\n",
    "Here we quantize the demographic and survey feature data based on the median. \n",
    "\n",
    "For both Q-Learning and the GSMRL model we use only the features that are markers for the target outputs.\n",
    "In this case, there are 30 survey DPDS features and 20 demographic features. We assume that the 20 demographic features are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b25298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float first\n",
    "features = features.astype(float)\n",
    "ind_demo = ind_demo.astype(float)\n",
    "target = target.astype(float)\n",
    "features_col = features.columns\n",
    "ind_demo_col = ind_demo.columns\n",
    "target_col = target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481860e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Survey Features: Index(['DPDS19', 'Determined', 'DPDS29', 'DPDS02', 'Indifferent', 'Severe2',\n",
      "       'DPDS11', 'DPDS09', 'SympatheticIP', 'DPDS07', 'DPDS22', 'DPDS27',\n",
      "       'DPDS25', 'Hostile', 'DPDS01', 'Alert', 'DPDS28', 'DPDS30', 'DPDS03',\n",
      "       'Assertive'],\n",
      "      dtype='object')\n",
      "Previous Demographic Features Index(['RigidPerfectionism', 'NEO', 'Race2', 'Gender', 'IIPSCMP', 'ISC_HI',\n",
      "       'Relationships', 'Depressivity', 'IIPSC_DE', 'Submissiveness',\n",
      "       'IIPSC_ZBC', 'ISC_PA', 'Substance', 'NEO_N', 'ISC_NO', 'Eccentricity',\n",
      "       'PNI_SSSE', 'PNI_DEV', 'SWLmean', 'PNI_EXP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Previous Survey Features and Demographic Features\n",
    "top_20 = [54, 19, 64, 37, 26,  3, 46, 44, 88, 42, 57, 62, 60, 20, 36, 16, 63, 65, 38, 24]\n",
    "top_demo = [48, 98,  4,  0, 91, 67, 16, 29, 74, 28, 81, 63, 23, 95, 70, 50, 55, 58, 71, 54]\n",
    "print(\"Previous Survey Features:\", features_col[top_20])\n",
    "print(\"Previous Demographic Features\", ind_demo_col[top_demo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49cc9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get marker demographic features\n",
    "marker_demo = ['EmotionalLability', 'Anxiousness', 'Depressivity', \n",
    "              'SeparationInsecurity', 'Anhedonia', 'Withdrawal',\n",
    "              'RestrictedAffectivity', 'AttentionSeeking', 'Grandiosity',\n",
    "              'IntimacyAvoidance', 'Hostility', 'Manipulativeness',\n",
    "              'Deceitfulness', 'Impulsivity', 'RiskTaking',\n",
    "              'Irresponsibility', 'RigidPerfectionism', 'PerceptualDysregulation',\n",
    "              'Eccentricity', 'Suspiciousness']\n",
    "demo_marker = ind_demo.loc[:, marker_demo]\n",
    "Q_demo = demo_marker.to_numpy()\n",
    "Q_demo = Q_demo.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da960c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize demographic features\n",
    "for i in range(Q_demo.shape[1]):\n",
    "    median = np.median(Q_demo[:,i])\n",
    "    zeros = Q_demo[:,i] - median <= 1e-3\n",
    "    Q_demo[zeros,i] = 0.\n",
    "    Q_demo[np.logical_not(zeros),i] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb0e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize Survey Features\n",
    "marker_features = ['DPDS01', 'DPDS02', 'DPDS03', 'DPDS04', 'DPDS05', 'DPDS06',\n",
    "                  'DPDS07', 'DPDS08', 'DPDS09', 'DPDS10', 'DPDS11', 'DPDS12',\n",
    "                  'DPDS13', 'DPDS15', 'DPDS16', 'DPDS17', 'DPDS19', 'DPDS20', \n",
    "                  'DPDS21', 'DPDS22', 'DPDS23', 'DPDS24', 'DPDS25', 'DPDS26', \n",
    "                  'DPDS27', 'DPDS28', 'DPDS29', 'DPDS30', 'DPDS31', 'DPDS32']\n",
    "features_marker = features.loc[:,marker_features]\n",
    "Q_features = features_marker.to_numpy()\n",
    "Q_features = Q_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af25f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize survey DPDS features\n",
    "for i in range(Q_features.shape[1]):\n",
    "    median = np.median(Q_features[:,i])\n",
    "    zeros = Q_features[:,i] - median <= 1e-3\n",
    "    Q_features[zeros,i] = 0.\n",
    "    Q_features[np.logical_not(zeros),i] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba503a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize Target Outputs\n",
    "Q_target = target.to_numpy()\n",
    "Q_target = Q_target.astype(np.float32)\n",
    "for i in range(Q_target.shape[1]):\n",
    "    median = np.median(Q_target[:,i])\n",
    "    zeros = Q_target[:,i] - median <= 1e-3\n",
    "    Q_target[zeros,i] = 0.\n",
    "    Q_target[np.logical_not(zeros),i] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f52ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Q-Table Features and Target for each Target Output\n",
    "\n",
    "for i in range(len(target_col)):\n",
    "    Q_table_data = {\n",
    "        'train': (Q_features[train_indices], Q_target[train_indices,i]),\n",
    "        'valid': (Q_features[validation_indices], Q_target[validation_indices,i]),\n",
    "        'test': (Q_features[test_indices], Q_target[test_indices,i])\n",
    "    }\n",
    "    with open(f\"./Q-table-data/psych_{target_col[i]}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(Q_table_data, f)\n",
    "\n",
    "\n",
    "Q_table_data = {\n",
    "    'train': (Q_features[train_indices], Q_target[train_indices]),\n",
    "    'valid': (Q_features[validation_indices], Q_target[validation_indices]),\n",
    "    'test': (Q_features[test_indices], Q_target[test_indices])\n",
    "}\n",
    "with open(\"./Q-table-data/psych_total.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q_table_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae39a3",
   "metadata": {},
   "source": [
    "# Prepare Data for GSMRL Model\n",
    "We add noise to all survey features and target outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f72e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851723c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "for i in range(1,93):\n",
    "    train_indices += df.index[df['ID'] == i].tolist()\n",
    "validation_indices = []\n",
    "for i in range(93,105):\n",
    "    validation_indices += df.index[df['ID'] == i].tolist()\n",
    "test_indices = []\n",
    "for i in range(105,117):\n",
    "    test_indices += df.index[df['ID'] == i].tolist()\n",
    "\n",
    "train_indices      = np.array(train_indices, dtype=np.int64)\n",
    "validation_indices = np.array(validation_indices, dtype=np.int64)\n",
    "test_indices       = np.array(test_indices, dtype=np.int64)\n",
    "\n",
    "if train_indices.shape[0]%window != 0:\n",
    "    train_indices = train_indices[:-(train_indices.shape[0]%window)]\n",
    "if validation_indices.shape[0]%window != 0:\n",
    "    validation_indices = validation_indices[:-(validation_indices.shape[0]%window)]\n",
    "if test_indices.shape[0]%window != 0:\n",
    "    test_indices = test_indices[:-(test_indices.shape[0]%window)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "261f1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFA_features = features_marker.to_numpy()\n",
    "AFA_target = target.to_numpy()\n",
    "AFA_features = AFA_features.astype(np.float32)\n",
    "AFA_target = AFA_target.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d977b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFA_feat_train = AFA_features[train_indices]\n",
    "AFA_feat_valid = AFA_features[validation_indices]\n",
    "AFA_feat_test  = AFA_features[test_indices]\n",
    "AFA_target_train = AFA_target[train_indices]\n",
    "AFA_target_valid = AFA_target[validation_indices]\n",
    "AFA_target_test  = AFA_target[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff81f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat   = np.zeros((train_indices.shape[0]//window, AFA_features.shape[1]*window))\n",
    "train_target = np.zeros((train_indices.shape[0]//window, AFA_target.shape[1]*window))\n",
    "valid_feat   = np.zeros((validation_indices.shape[0]//window, AFA_features.shape[1]*window))\n",
    "valid_target = np.zeros((validation_indices.shape[0]//window, AFA_target.shape[1]*window))\n",
    "test_feat    = np.zeros((test_indices.shape[0]//window, AFA_features.shape[1]*window))\n",
    "test_target  = np.zeros((test_indices.shape[0]//window, AFA_target.shape[1]*window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddce1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, train_indices.shape[0], window):\n",
    "    train_feat[i//window]   = AFA_feat_train[i:i+window].flatten()\n",
    "    train_target[i//window] = AFA_target_train[i:i+window].flatten()\n",
    "\n",
    "for i in range(0, validation_indices.shape[0], window):\n",
    "    valid_feat[i//window]   = AFA_feat_valid[i:i+window].flatten()\n",
    "    valid_target[i//window] = AFA_target_valid[i:i+window].flatten()\n",
    "    \n",
    "for i in range(0, test_indices.shape[0], window):\n",
    "    test_feat[i//window]   = AFA_feat_test[i:i+window].flatten()\n",
    "    test_target[i//window] = AFA_target_test[i:i+window].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75ff66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat   += np.random.normal(0, 0.02, train_feat.shape)\n",
    "train_target += np.random.normal(0, 0.02, train_target.shape)\n",
    "valid_feat   += np.random.normal(0, 0.02, valid_feat.shape)\n",
    "valid_target += np.random.normal(0, 0.02, valid_target.shape)\n",
    "test_feat    += np.random.normal(0, 0.02, test_feat.shape)\n",
    "test_target  += np.random.normal(0, 0.02, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f817a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save AFA Features and Target for each Target Output\n",
    "AFA_data = {\n",
    "    'train': (train_feat.astype(np.float32), train_target.astype(np.float32)),\n",
    "    'valid': (valid_feat.astype(np.float32), valid_target.astype(np.float32)),\n",
    "    'test':  (test_feat.astype(np.float32),  test_target.astype(np.float32)),\n",
    "}\n",
    "with open(f\"./psych_GSMRL.pkl\", \"wb\") as f:\n",
    "    pickle.dump(AFA_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cdea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
